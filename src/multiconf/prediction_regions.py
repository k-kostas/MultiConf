import torch
import numpy as np
import pandas as pd
import warnings

from typing import Tuple, List, Dict, Union, Optional
from scipy.stats import kstest

from .utils import _check_multihot_labels, _is_tensor


PredictionItem = Union[Tuple[torch.Tensor, float], torch.Tensor]
SingleAlphaOutput = List[PredictionItem]
MultiAlphaOutput = Dict[float, SingleAlphaOutput]
PredictionOutput = Union[SingleAlphaOutput, MultiAlphaOutput]


class PredictionRegions:
    """
    A container class for conformal prediction results.

    This class wraps the p-values generated by the Inductive Conformal Predictor
    and provides methods to extract prediction sets at specific significance levels
    and evaluate performance metrics.


    Parameters
    ----------
    p_values : torch.Tensor
        The p-values associated with every possible label combination for each test sample.
        Shape: (n_samples, n_classes).
    combinations : torch.Tensor
        The matrix of all possible label combinations corresponding to the columns of `p_values`.
        Shape: (2^n_classes, n_classes).


    Examples
    --------
    >>> import torch
    >>> from multiconf.prediction_regions import PredictionRegions
    >>>
    >>> # 1. Generate dummy data (100 samples, 5 classes -> 32 combinations)
    >>> combinations = torch.cartesian_prod(*[torch.tensor([0, 1])] * 5)
    >>> p_values = torch.rand(100, 5)
    >>>
    >>> # 2. Initialize container
    >>> prediction_regions_obj = PredictionRegions(p_values, combinations)
    """

    def __init__(self, p_values: torch.Tensor, combinations: torch.Tensor):
        self.p_values = p_values
        self.combinations = combinations
        self.device = p_values.device


    def get_valid_tuples(self, alpha: float) -> List[torch.Tensor]:
        """
        Extracts valid label combinations for a specific significance level.

        A combination is considered valid if its p-value is greater than the significance level `alpha`.

        Parameters
        ----------
        alpha : float
            The significance level (error rate). Confidence level = 1 - alpha.


        Returns
        -------
        List[torch.Tensor]
            A list where each element corresponds to a test sample. Each element is a Tensor
            containing the valid label combinations for that sample.

            - If `row_counts` is 0 (empty set), the combination with the highest p-value is returned
              to ensure non-empty predictions.
        """

        mask = self.p_values > alpha
        row_counts = mask.sum(dim=1)
        empty_mask = (row_counts == 0)
        if empty_mask.any():
            max_indices = self.p_values.argmax(dim=1)
            mask[empty_mask, max_indices[empty_mask]] = True
            row_counts[empty_mask] = 1
        valid_indices = mask.nonzero()
        comb_indices = valid_indices[:, 1]
        flat_predictions = self.combinations[comb_indices]

        return list(torch.split(flat_predictions.int(), row_counts.cpu().tolist()))


    def get_true_label_p_value(self, true_label) -> torch.Tensor:
        """
        Retrieves the p-values assigned to the actual ground truth labels.


        Parameters
        ----------
        true_label : torch.Tensor
            The ground truth labels for the test samples.
            Shape: (n_samples, n_classes).


        Returns
        -------
        torch.Tensor
            A 1D tensor of p-values corresponding to the true labels.
            Shape: (n_samples,).
        """

        matches = (true_label.unsqueeze(1) == self.combinations.unsqueeze(0)).all(dim=-1)

        return self.p_values[matches]


    def check_ks_test(self, true_labels) -> Dict[str, float]:
        """
        Performs a Kolmogorov-Smirnov test.

        Tests if the p-values of the true labels follow a Uniform(0, 1) distribution.


        Parameters
        ----------
        true_labels : torch.Tensor
            The ground truth labels.


        Returns
        -------
        Dict[str, float]
            A dictionary containing:
            - 'ks_statistic': The KS statistic.
            - 'ks_p_value': The p-value of the KS test (should be > 0.05 for validity).
            - 'is_valid': Boolean indicating if the null hypothesis (uniformity) is not rejected.
        """

        matches = (true_labels.unsqueeze(1) == self.combinations.unsqueeze(0)).all(dim=-1)
        true_p_values = self.p_values[matches]

        if len(true_p_values) < 30:
            warnings.warn(f"KS-Test running with only {len(true_p_values)} samples. Results may be unreliable.",
                          RuntimeWarning)

        true_p_values_np = true_p_values.detach().cpu().numpy()
        ks_stat, ks_pval = kstest(true_p_values_np, 'uniform')

        return {
            "ks_statistic": ks_stat,
            "ks_p_value": ks_pval,
            "is_valid": ks_pval > 0.05
        }

    @torch.no_grad()
    def __call__(self,
                 significance_level: Optional[Union[float, List[float], np.ndarray, pd.Series, torch.Tensor]] = None
                 ) -> PredictionOutput:
        """
        Extracts prediction sets for one or multiple significance levels.


        Parameters
        ----------
        significance_level : float or List[float]
            The significance level(s) (alpha) for which to return prediction regions.
            Must be strictly between 0 and 1.


        Returns
        -------
        Union[List[torch.Tensor], Dict[float, List[torch.Tensor]]]
            - If ``significance_level`` is a scalar: Returns a list of valid tuples.
            - If ``significance_level`` is a list: Returns a dictionary mapping alpha values to lists of valid tuples.


        Raises
        ------
        ValueError
            If ``significance_level`` is None or out of range [0, 1].


        Examples
        --------
        >>> # Scalar alpha
        >>> prediction_sets_lst = prediction_regions_obj(0.1)
        >>>
        >>> # Multiple alphas
        >>> prediction_sets_dict = prediction_regions_obj([0.05, 0.1])
        """

        if significance_level is None:
            raise ValueError("significance_level must be specified")
        else:
            print(f'Returning prediction regions for significance level {significance_level}.')

        is_scalar = False
        if torch.is_tensor(significance_level):
            if significance_level.ndim == 0:
                is_scalar = True
                alphas = [significance_level.item()]
            else:
                alphas = significance_level.tolist()
        elif isinstance(significance_level, np.ndarray):
            if significance_level.ndim == 0:
                is_scalar = True
                alphas = [significance_level.item()]
            else:
                alphas = significance_level.tolist()
        elif isinstance(significance_level, pd.Series):
            alphas = significance_level.tolist()
        elif isinstance(significance_level, (list, tuple)):
            alphas = list(significance_level)
        else:
            is_scalar = True
            alphas = [significance_level]
        alphas = [round(float(a), 3) for a in alphas]

        for a in alphas:
            if not (0 <= a <= 1):
                raise ValueError(f"Significance level must be strictly between 0 and 1, got {a}")

        results = {}
        for alpha in alphas:
            results[alpha] = self.get_valid_tuples(alpha)

        if is_scalar:
            return results[alphas[0]]
        else:
            return results

    @torch.no_grad()
    def evaluate(self,
                 return_true_label_p_value: bool = True,
                 return_coverage: bool = True,
                 return_n_criterion: bool = True,
                 return_s_criterion: bool = True,
                 return_ks_test: bool = True,
                 *,
                 true_labelsets: Optional[Union[np.ndarray, torch.Tensor, pd.DataFrame, pd.Series, List]]=None,
                 significance_level: Optional[Union[float, List[float]]] = None,
                 ) -> Optional[Dict]:
        """
        Evaluates the conformal predictor using standard metrics.

        Calculates validity (coverage, KS-test) and efficiency (N-criterion, S-criterion) metrics.


        Parameters
        ----------
        return_true_label_p_value : bool, default=True
            Whether to return the p-values of the true class (requires `true labelsets`).
        return_coverage : bool, default=True
            Whether to calculate empirical coverage (requires `true labelsets` and `significance_level`).
        return_n_criterion : bool, default=True
            Whether to calculate average set size (N-criterion) (requires `true labelsets` and `significance_level`).
        return_s_criterion : bool, default=True
            Whether to calculate the S-criterion (sum of p-values), a threshold-independent efficiency metric.
        return_ks_test : bool, default=True
            Whether to perform the KS-test for uniformity (requires `true labelsets`).
        true_labelsets : torch.Tensor or array-like, optional
            The ground truth labels. Required for coverage and p-value metrics.
        significance_level : float or List[float], optional
            The alpha level(s) to evaluate coverage and N-criterion (set size).


        .. note::
            The ``true_labelsets`` and the ``significance_level`` must be passed as a keyword argument.


        Returns
        -------
        Dict
            A dictionary containing the requested metrics. If ``significance_level`` is a list,
            returns a dictionary where keys are alphas and values are metric dictionaries.


        Raises
        ------
        ValueError
            If ``true_labelsets`` shape does not match the number of classes.


        Examples
        --------
        >>> # Generate dummy test labels
        >>> y_test = torch.rand(10, 5)
        >>>
        >>> # Evaluate specific alpha
        >>> metrics = prediction_regions_obj.evaluate(true_labelsets=y_test, significance_level=0.1)
        >>>
        >>> # Evaluate multiple alphas
        >>> metrics_multi = prediction_regions_obj.evaluate(true_labelsets=y_test, significance_level=[0.05, 0.1])
        """

        if true_labelsets is not None:
            true_labelsets = _check_multihot_labels(true_labelsets)
            true_labelsets = _is_tensor(true_labelsets).to(self.device)

            if true_labelsets.shape[1] != self.combinations.shape[1]:
                raise ValueError("True labels must have the same number of columns as the number of classes.")

            if significance_level is not None:
                predictions = self(significance_level)

                def _evaluate_metrics(preds_list, targets):
                    total = len(targets)
                    covered = 0
                    size_sum = 0

                    for i in range(total):
                        valid_combs = preds_list[i]
                        current_size = len(valid_combs)

                        if return_n_criterion:
                            size_sum += current_size

                        if return_coverage and current_size > 0:
                            target = targets[i]
                            matches = (valid_combs == target).all(dim=1)
                            if matches.any():
                                covered += 1

                    out = {}
                    if return_true_label_p_value:
                        out['true_labels_p_values'] = self.get_true_label_p_value(true_labelsets)

                    if return_coverage:
                        out['coverage'] = covered / total if total > 0 else 0.0

                    if return_n_criterion:
                        out['n_criterion'] = size_sum / total if total > 0 else 0.0

                    if return_s_criterion:
                        out['s_criterion'] = self.p_values.sum(dim=1).mean().item()

                    if return_ks_test:
                        out['ks_test_metrics'] = self.check_ks_test(true_labelsets)

                    return out

                if isinstance(predictions, dict):
                    return {alpha: _evaluate_metrics(preds, true_labelsets) for alpha, preds in predictions.items()}
                else:
                    return _evaluate_metrics(predictions, true_labelsets)

            else:

                if true_labelsets.shape[1] != self.combinations.shape[1]:
                    raise ValueError("True labels must have the same number of columns as the number of classes.")

                out = {}
                if return_true_label_p_value:
                    out['true_labels_p_values'] = self.get_true_label_p_value(true_labelsets)

                if return_coverage:
                    warnings.warn("Returning coverage is not supported when significance_level is not provided.", RuntimeWarning)

                if return_n_criterion:
                    warnings.warn("Returning n-criterion is not supported when significance_level is not provided.", RuntimeWarning)

                if return_s_criterion:
                    out['s_criterion'] = self.p_values.sum(dim=1).mean().item()

                if return_ks_test:
                    out['ks_test_metrics'] = self.check_ks_test(true_labelsets)

                return out

        else:

            out = {}
            if return_true_label_p_value:
                warnings.warn("Returning true label p-values is not supported when true labels are not provided.", RuntimeWarning)

            if return_coverage:
                warnings.warn("Returning coverage is not supported when true labels and significance_level are not provided.", RuntimeWarning)

            if return_n_criterion:
                warnings.warn("Returning n-criterion is not supported when true labels and significance_level are not provided.", RuntimeWarning)

            if return_s_criterion:
                out['s_criterion'] = self.p_values.sum(dim=1).mean().item()

            if return_ks_test:
                warnings.warn("Returning ks-test metrics is not supported when true labels are not provided.", RuntimeWarning)

            return out